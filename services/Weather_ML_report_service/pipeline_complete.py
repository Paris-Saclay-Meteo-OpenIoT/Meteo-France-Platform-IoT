#!/usr/bin/env python3
"""
Wrapper pour synchroniser MongoDB ‚Üí PostgreSQL avant d'ex√©cuter le pipeline ML
G√®re les donn√©es m√©t√©orologiques et g√©n√®re les pr√©dictions
"""

import logging
import sys
import os
import asyncio
from mongo_to_postgres import main as sync_mongodb_to_postgres
from main import run_pipeline
from stations_config import get_target_stations, AVAILABLE_STATIONS

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)

logger = logging.getLogger(__name__)

def run_full_pipeline(target_stations=None, region=None):
    """
    Ex√©cute la synchronisation puis le pipeline ML
    
    Args:
        target_stations: Liste explicite des stations √† traiter
        region: R√©gion pr√©d√©finie ('CORSE', 'PROVENCE_ALPES_AZUR', etc.)
    """
    # D√©terminer les stations cibles
    # NOTE: Si pas de param√®tres, on traite TOUTES les stations disponibles
    if region:
        target_stations = get_target_stations(region)
        logger.info(f"üìç R√©gion s√©lectionn√©e: {region}")
    elif target_stations is None:
        # Par d√©faut: traiter TOUTES les stations (None = pas de filtrage)
        target_stations = None
        logger.info(f"üìç Pas de filtrage - Toutes les stations seront trait√©es")
    
    try:
        logger.info("‚ñà" * 80)
        logger.info("üåç D√âMARRAGE DU PIPELINE M√âT√âOROLOGIQUE COMPLET")
        logger.info("‚ñà" * 80)
        if target_stations:
            logger.info(f"üìç Stations cibles ({len(target_stations)}):")
            for station in sorted(target_stations):
                station_id = AVAILABLE_STATIONS.get(station, 'N/A')
                logger.info(f"   ‚Ä¢ {station:15s} (ID: {station_id})")
        else:
            logger.info(f"üìç Traitement de TOUTES les stations disponibles")
        logger.info("‚ñà" * 80)
        
        # √âtape 0: R√©cup√©ration / mise √† jour des donn√©es historiques via API DPClim
        # Ex√©cut√© √† chaque changement de jour pour maintenir les donn√©es √† jour
        api_token_clima = os.getenv("API_TOKEN_CLIMA")
        if api_token_clima:
            logger.info("\n")
            logger.info("‚ñì" * 80)
            logger.info("üì° √âTAPE 0: Mise √† jour des donn√©es historiques DPClim")
            logger.info("‚ñì" * 80)
            try:
                from fetch_historical_data import main_async
                from sqlalchemy import create_engine, text
                from config import DB_URI
                
                engine = create_engine(DB_URI)
                with engine.connect() as conn:
                    result = conn.execute(text(
                        "SELECT COUNT(*), MAX(date) FROM weather_data"
                    ))
                    row = result.fetchone()
                    count = row[0] if row else 0
                    max_date = row[1] if row else None
                engine.dispose()
                
                from datetime import datetime, timedelta
                now = datetime.utcnow()
                
                if count < 5000:
                    # Premi√®re ex√©cution : r√©cup√©rer 3 mois d'historique (suffisant pour le ML)
                    logger.info(f"   üìä Seulement {count} enregistrements en base"
                                " ‚Äî lancement de la r√©cup√©ration historique (3 mois)")
                    asyncio.run(main_async(months=3))
                elif max_date and (now - max_date).total_seconds() > 24 * 3600:
                    # Donn√©es obsol√®tes : r√©cup√©rer le dernier mois pour combler le gap
                    days_behind = (now - max_date).days
                    logger.info(f"   üìä {count} enregistrements, derni√®re donn√©e: {max_date.strftime('%Y-%m-%d %H:%M')}"
                                f" ({days_behind}j de retard) ‚Äî mise √† jour incr√©mentale")
                    asyncio.run(main_async(months=1))
                else:
                    logger.info(f"   ‚úÖ {count} enregistrements, donn√©es √† jour"
                                f" (derni√®re: {max_date.strftime('%Y-%m-%d %H:%M') if max_date else 'N/A'})")
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è R√©cup√©ration historique non-bloquante: {e}")
        else:
            logger.info("   ‚ÑπÔ∏è  API_TOKEN_CLIMA non d√©fini ‚Äî historique DPClim ignor√©")
        
        # √âtape 1: Synchronisation
        logger.info("\n")
        logger.info("‚ñì" * 80)
        logger.info("üîÑ √âTAPE 1: Synchronisation MongoDB ‚Üí PostgreSQL")
        logger.info("‚ñì" * 80)
        try:
            sync_result = sync_mongodb_to_postgres()
            logger.info("‚úÖ Synchronisation r√©ussie")
            logger.info("‚ñì" * 80)
        except Exception as e:
            logger.error(f"‚ùå Erreur synchronisation: {e}", exc_info=True)
            raise
        
        # √âtape 2: Pipeline ML et pr√©dictions
        logger.info("\n")
        logger.info("‚ñì" * 80)
        logger.info("ü§ñ √âTAPE 2: Ex√©cution du Pipeline ML - G√©n√©ration des pr√©dictions")
        logger.info("‚ñì" * 80)
        try:
            ml_result = run_pipeline(target_stations=target_stations)
            logger.info("‚úÖ Pipeline ML r√©ussi")
            logger.info("‚ñì" * 80)
        except Exception as e:
            logger.error(f"‚ùå Erreur Pipeline ML: {e}", exc_info=True)
            raise
        
        # R√©sum√© final
        logger.info("\n")
        logger.info("‚ñà" * 80)
        logger.info("üéâ ‚úÖ PIPELINE COMPLET R√âUSSI AVEC SUCC√àS!")
        logger.info("‚ñà" * 80)
        logger.info("üìä Les pr√©dictions sont maintenant disponibles:")
        logger.info("   ‚Ä¢ Table PostgreSQL: forecast_results")
        logger.info("   ‚Ä¢ Accessible via API backend: /api/station/forecast/*")
        logger.info("   ‚Ä¢ Affichage frontend: Dashboard des stations")
        logger.info("‚ñà" * 80)
        
    except Exception as e:
        logger.error("\n")
        logger.error("‚ñà" * 80)
        logger.error(f"üí• ERREUR CRITIQUE DANS LE PIPELINE COMPLET: {e}")
        logger.error("‚ñà" * 80)
        raise

if __name__ == "__main__":
    # Lire la r√©gion depuis une variable d'environnement si disponible
    region = os.getenv('ML_TARGET_REGION', None)
    
    logger.info(f"R√©gion cible (env ML_TARGET_REGION): {region or 'D√©faut (toutes les stations)'}")
    
    run_full_pipeline(region=region)
