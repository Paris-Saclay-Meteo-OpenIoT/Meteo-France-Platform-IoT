networks:
  pipeline-net:
    driver: bridge

services:
  #-------------------------------------------------------
  # Nginx
  #-------------------------------------------------------
  nginx:
    image: "jc21/nginx-proxy-manager:latest"
    container_name: ter_nginx_proxy_manager
    ports:
      # ports <host-port>:<container-port>
      - "80:80" # Public HTTP Port
      - "443:443" # Public HTTPS Port
      - "81:81" # Admin Web Port
    volumes:
      - npm_data:/data #create vol in the container at /data path
      - npm_letsencrypt:/etc/letsencrypt

    networks:
      - pipeline-net
    restart: unless-stopped

  #-------------------------------------------------------
  # Kafka & Zookeeper
  #-------------------------------------------------------

  zookeeper:
    image: wurstmeister/zookeeper
    container_name: ter_zookeeper
    ports:
      - "2181:2181"
    volumes:
      - ter_zookeeper_data:/data
      - ./logs/ter_zookeeper_datalog:/datalog
    networks:
      - pipeline-net
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "echo", "ruok", "|", "nc", "localhost", "2181"]
      interval: 5s
      timeout: 5s
      retries: 3

  kafka:
    image: wurstmeister/kafka
    container_name: ter_kafka
    ports:
      - "9092:9092"
    environment:
      - KAFKA_ZOOKEEPER_CONNECT=ter_zookeeper:2181
      - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://ter_kafka:9092
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092
    volumes:
      - ter_kafka_data:/var/lib/kafka
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "kafka-topics.sh",
          "--list",
          "--bootstrap-server",
          "localhost:9092",
        ]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - pipeline-net
    depends_on:
      zookeeper:
        condition: service_healthy

  kafka-topic-init:
    image: wurstmeister/kafka
    container_name: kafka_topic_initializer
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - pipeline-net
    environment:
      - KAFKA_ZOOKEEPER_CONNECT=ter_zookeeper:2181
    volumes:
      - ./infrastructure/kafka:/init_scripts
    healthcheck:
      test:
        [
          "CMD",
          "kafka-topics.sh",
          "--list",
          "--bootstrap-server",
          "ter_kafka:9092",
        ]
      interval: 5s
      timeout: 10s
      retries: 3
    restart: "no"
    entrypoint:
      ["/bin/sh", "-c", "sh /init_scripts/init_kafka_topics.sh || true"]

  kafka-ui:
    container_name: ter_kafka-ui
    image: provectuslabs/kafka-ui:latest
    ports:
      - "8090:8090"
    environment:
      SERVER_PORT: 8090
      KAFKA_CLUSTERS_0_NAME: "weather-cluster"
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: "ter_kafka:9092"
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - pipeline-net
    restart: unless-stopped

  #-------------------------------------------------------
  # Databases
  #-------------------------------------------------------

  #mongodb:
  #  image: mongo:5.0
  #  container_name: ter_mongodb
  #  ports:
  #    - "27017:27017"
  #  environment:
  #    MONGO_INITDB_ROOT_USERNAME: ${MONGO_INITDB_ROOT_USERNAME}
  #    MONGO_INITDB_ROOT_PASSWORD: ${MONGO_INITDB_ROOT_PASSWORD}
  #  volumes:
  #    - ter_mongodb_data:/data/db
  #  depends_on:
  #   kafka-topic-init:
  #      condition: service_completed_successfully
  #  networks:
  #    - pipeline-net
  #  healthcheck:
  #    test: ["CMD", "mongo", "--eval", "db.adminCommand('ping')"]
  #    interval: 5s
  #    timeout: 5s
  #    retries: 3

  redis:
    image: redis:latest
    container_name: ter_redis
    user: root
    ports:
      - "6379:6379"
    restart: unless-stopped
    command:
      [
        "redis-server",
        "--requirepass",
        "${REDIS_PASSWORD}",
        "--notify-keyspace-events",
        "KEA",
      ]
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 5s
      timeout: 5s
      retries: 3
    #depends_on:
    #kafka-topic-init:
    #condition: service_completed_successfully
    networks:
      - pipeline-net

  postgres:
    image: postgres:13
    container_name: ter_postgres
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "5432:5432"
    volumes:
      - ter_postgres_data:/var/lib/postgresql/data
      - ./infrastructure/postgres:/docker-entrypoint-initdb.d
    networks:
      - pipeline-net
    healthcheck:
      test:
        ["CMD", "pg_isready", "-U", "${POSTGRES_USER}", "-d", "${POSTGRES_DB}"]
      interval: 5s
      timeout: 5s
      retries: 3
  #-------------------------------------------------------
  # API Producers
  #-------------------------------------------------------

  api_observations_producer:
    build:
      context: ./services/api_observations_producer
    container_name: ter_api_observations_producer
    volumes:
      - ./services/api_observations_producer:/app
      - ./logs/api_observations:/app/logs
      - ./services/utils/batches.json:/app/utils/batches.json
    environment:
      - KAFKA_BROKER=${KAFKA_BROKER}
      - API_TOKEN=${API_TOKEN_OBS}
    depends_on:
      kafka-topic-init:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
    networks:
      - pipeline-net
    # TEST
    # command: ["pytest", "--maxfail=5", "--disable-warnings", "-v"]

  api_climatologique_producer:
    build:
      context: ./services/api_climatologique_producer
    container_name: ter_api_climatologique_producer
    volumes:
      - ./services/api_climatologique_producer:/app
      - ./logs/climatologique:/app/logs
      - ./services/utils/batches_corse.json:/app/utils/batches_corse.json
    environment:
      - KAFKA_BROKER=${KAFKA_BROKER}
      - API_TOKEN=${API_TOKEN_CLIMA}
    depends_on:
      kafka-topic-init:
        condition: service_completed_successfully
    #    mongodb:
    #      condition: service_healthy
    networks:
      - pipeline-net

  api_vigilance_producer:
    build:
      context: ./services/api_vigilance_producer
    container_name: ter_api_vigilance_producer
    volumes:
      - ./services/api_vigilance_producer:/app
      - ./logs/vigilance:/app/logs
    environment:
      - KAFKA_BROKER=${KAFKA_BROKER}
      - API_TOKEN=${API_TOKEN_VIGILANCE}
    depends_on:
      kafka-topic-init:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
    networks:
      - pipeline-net

  #-------------------------------------------------------
  # Consumers ==> ETL
  #-------------------------------------------------------

  # postgres_consumer:
  #  build:
  #    context: ./services/mongo_consumer
  #  container_name: ter_postgres_consumer
  #  volumes:
  #    - ./services/mongo_consumer:/app
  #  environment:
  #    - KAFKA_BROKER=${KAFKA_BROKER}
  #    - POSTGRES_USER=${POSTGRES_USER}
  #    - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
  #    - POSTGRES_DB=${POSTGRES_DB}
  #    - POSTGRES_HOST=ter_postgres
  #    - POSTGRES_PORT=5432
  #  depends_on:
  #    - kafka
  #    - postgres
  #  networks:
  #    - pipeline-net
  #  restart: unless-stopped

  # lien https obligatoire : https://localhost:8443/
  nifi:
    image: apache/nifi:2.0.0
    container_name: ter_nifi
    ports:
      - "8443:8443"
    environment:
      - NIFI_WEB_HTTPS_PORT=8443
      - NIFI_WEB_HTTPS_HOST=0.0.0.0
      - SINGLE_USER_CREDENTIALS_USERNAME=admin
      - SINGLE_USER_CREDENTIALS_PASSWORD=MeteoFrance2026!
      - NIFI_SENSITIVE_PROPS_KEY=ter-nifi-secret-key-very-long

    entrypoint:
      - /bin/bash
      - -c
      - |
        set -e
        echo "=== 1. PREPARATION ==="
        ls -l /opt/nifi/drivers/postgresql-42.7.2.jar && echo "Driver Postgres TROUVÃ‰ via volume." || echo "ERREUR: Driver introuvable !"

        echo "=== 2. PROVISIONING FLUX ==="
        cp -v /tmp/flow_init.json.gz /opt/nifi/nifi-current/conf/flow.json.gz
        
        echo "=== 3. DEMARRAGE ==="
        ../scripts/start.sh

    volumes:
      - nifi_conf:/opt/nifi/nifi-current/conf
      - nifi_database:/opt/nifi/nifi-current/database_repository
      - nifi_flowfile:/opt/nifi/nifi-current/flowfile_repository
      - nifi_content:/opt/nifi/nifi-current/content_repository
      - nifi_provenance:/opt/nifi/nifi-current/provenance_repository
      - nifi_state:/opt/nifi/nifi-current/state
      - nifi_logs:/opt/nifi/nifi-current/logs
      
      # 1. Golden Copy
      - ./infrastructure/nifi/flow/flow_gold.json.gz:/tmp/flow_init.json.gz:ro
      
      # 2. DRIVERS
      - ./infrastructure/nifi/drivers:/opt/nifi/drivers:ro
      
    depends_on:
      - kafka
    restart: unless-stopped
    networks:
      - pipeline-net

  # mongo_consumer:
  #   build:
  #     context: ./services/mongo_consumer
  #   container_name: ter_mongo_consumer
  #   volumes:
  #     - ./services/mongo_consumer:/app
  #   environment:
  #     - KAFKA_BROKER=${KAFKA_BROKER}
  #     - MONGO_INITDB_ROOT_USERNAME=${MONGO_INITDB_ROOT_USERNAME}
  #     - MONGO_INITDB_ROOT_PASSWORD=${MONGO_INITDB_ROOT_PASSWORD}
  #   depends_on:
  #     - kafka
  #     - mongodb
  #   networks:
  #     - pipeline-net

  # redis_consumer:
  #   build:
  #     context: ./services/redis_consumer
  #   container_name: ter_redis_consumer
  #   volumes:
  #     - ./services/redis_consumer:/app
  #     - ./logs/redis:/app/logs
  #   environment:
  #     - REDIS_HOST=ter_redis
  #     - REDIS_PASSWORD=${REDIS_PASSWORD}
  #     - REDIS_PORT=6379
  #     - KAFKA_BROKER=${KAFKA_BROKER}
  #     - KAFKA_TOPICS=weather-real-time,weather-alerts
  #   depends_on:
  #     kafka-topic-init:
  #       condition: service_completed_successfully
  #     redis:
  #       condition: service_healthy
  #   networks:
  #     - pipeline-net


  #-------------------------------------------------------
  # GRAFANA & its Datasources
  #-------------------------------------------------------

  grafana:
    image: grafana/grafana:latest
    container_name: ter_grafana
    ports:
      - "3000:3000"
    networks:
      - pipeline-net
    volumes:
      - ter_grafana_data:/var/lib/grafana
      - ./infrastructure/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - ./infrastructure/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
      - ./infrastructure/grafana/provisioning/alerting:/etc/grafana/provisioning/alerting
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_SECURITY_ALLOW_EMBEDDING: "true"
      GF_AUTH_ANONYMOUS_ENABLED: "true"
      GF_AUTH_ANONYMOUS_ORG_ROLE: "Viewer"
    restart: unless-stopped

  # Prometheus & its Exporters
  prometheus:
    image: prom/prometheus:latest
    container_name: ter_prometheus
    volumes:
      - ./infrastructure/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ter_prometheus_data:/prometheus
      - ./logs/prometheus:/prometheus/logs
    ports:
      - "9090:9090"
    networks:
      - pipeline-net

  # Pyroscope : mandatory datasource
  pyroscope:
    image: pyroscope/pyroscope:latest
    container_name: ter_pyroscope
    ports:
      - "4040:4040"
    command:
      - "server"
    networks:
      - pipeline-net
    restart: unless-stopped

  prometheus-node-exporter:
    image: prom/node-exporter:latest
    container_name: ter_prometheus-node-exporter
    ports:
      - "9100:9100"
    networks:
      - pipeline-net
    restart: unless-stopped

  #prometheus-mongodb-exporter:
  #  image: bitnami/mongodb-exporter:latest
  #  container_name: ter_prometheus-mongodb-exporter
  #  ports:
  #    - "9216:9216"
  #  networks:
  #    - pipeline-net
  #  environment:
  #    MONGODB_URI: mongodb://mongodb:27017
  #    MONGODB_INITDB_ROOT_USERNAME: ${MONGO_INITDB_ROOT_USERNAME}
  #    MONGODB_INITDB_ROOT_PASSWORD: ${MONGO_INITDB_ROOT_PASSWORD}
  #  restart: unless-stopped

  prometheus-kafka-exporter:
    image: danielqsj/kafka-exporter:latest
    container_name: ter_prometheus-kafka-exporter
    ports:
      - "9308:9308"
    networks:
      - pipeline-net
    environment:
      KAFKA_SERVER: ter_kafka:9092
    restart: unless-stopped

  prometheus-redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: ter_prometheus-redis-exporter
    ports:
      - "9121:9121"
    networks:
      - pipeline-net
    environment:
      REDIS_ADDR: "ter_redis:6379"
      REDIS_PASSWORD: "${REDIS_PASSWORD}"
    restart: unless-stopped

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    container_name: ter_cadvisor
    privileged: true
    devices:
      - /dev/kmsg
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      # - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    ports:
      - "8080:8080"
    restart: unless-stopped
    networks:
      - pipeline-net
  #--------------------------------------------------------
  # AI
  #--------------------------------------------------------
  api-export-csv:
      build: ./services/api_export   
      container_name: ter_api_export_csv
      ports:
        - "8000:8000"
      environment:
        - POSTGRES_USER=my_user       
        - POSTGRES_PASSWORD=my_password
        - POSTGRES_DB=my_database
        - POSTGRES_HOST=ter_postgres  
        - POSTGRES_PORT=5432
      depends_on:
        - postgres  
      networks:
        - pipeline-net
      healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
        interval: 30s 
        timeout: 5s
        retries: 3

  api_anomaly_detection:
    build:
      context: ./services/api_anomaly_detection
    container_name: ter_api_anomaly_detection
    volumes:
      - ./services/api_anomaly_detection:/app
      - ./logs/anomaly_detection:/app/logs
    environment:
      - KAFKA_BROKER=${KAFKA_BROKER}
      - DETECTION_INTERVAL_HOURS=24
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - pipeline-net


  weather_ml_service:
    build: ./services/WEATHER_ML_REPORT_SERVICE
    container_name: weather_ml_report
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - pipeline-net  
    restart: always
    
  #-------------------------------------------------------
  # Client web
  #-------------------------------------------------------

  backend:
    build:
      context: ./services/backend
    container_name: ter_backend
    ports:
      - "5000:5000"

    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - MONGO_URI=mongodb://admin:password@ter_mongodb:27017/weatherDB?authSource=admin
      - MONGO_INITDB_ROOT_USERNAME=${MONGO_INITDB_ROOT_USERNAME}
      - MONGO_INITDB_ROOT_PASSWORD=${MONGO_INITDB_ROOT_PASSWORD}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_HOST=${POSTGRES_HOST}
      - POSTGRES_PORT=${POSTGRES_PORT}
      - JWT_SECRET=${JWT_SECRET}
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@ter_postgres:5432/${POSTGRES_DB}
      - EMAIL_ADDRESS=${EMAIL_ADDRESS}
      - EMAIL_PASSWORD=${EMAIL_PASSWORD}
      - CORS_ORIGIN=${CORS_ORIGIN}
    volumes:
      - ./services/backend:/usr/src/app
      - /usr/src/app/node_modules
    depends_on:
      #   mongodb:
      #      condition: service_healthy
      kafka-topic-init:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    networks:
      - pipeline-net

  frontend:
    build:
      context: ./services/frontend
    container_name: ter_frontend
    ports:
      - "3001:3001"
    environment:
      - ENV=${ENV}
      - NEXT_PUBLIC_API_URL=${ENVIRONMENT_URL}
      - NEXT_PUBLIC_GRAFANA_URL=${NEXT_PUBLIC_GRAFANA_URL}
      - PORT=3001
    volumes:
      - ./services/frontend:/usr/src/app
      - /usr/src/app/node_modules
    depends_on:
      - backend
    networks:
      - pipeline-net

volumes:
  npm_data:
  npm_letsencrypt:

  ter_mongodb_data:
  ter_kafka_data:
  ter_zookeeper_data:
  ter_zookeeper_datalog:
  ter_kafka_manager_logs:
  ter_prometheus_data:
  ter_grafana_data:
  ter_postgres_data:
  
  nifi_conf:
  nifi_database:
  nifi_flowfile:
  nifi_content:
  nifi_provenance:
  nifi_state:
  nifi_logs:

